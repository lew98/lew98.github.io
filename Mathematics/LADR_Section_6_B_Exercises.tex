\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{tabularray}
\usepackage{geometry}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{changepage}
\usepackage{lipsum}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{matrix}
\usepackage{tikz-cd}
\usepackage[nameinlink]{cleveref}
\geometry{
headheight=15pt,
left=60pt,
right=60pt
}
\setlength{\emergencystretch}{20pt}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{}
\chead{Section 6.B Exercises}
\rhead{\thepage}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue
}

\theoremstyle{definition}
\newtheorem*{remark}{Remark}

\newtheoremstyle{exercise}
    {}
    {}
    {}
    {}
    {\bfseries}
    {.}
    { }
    {\thmname{#1}\thmnumber{#2}\thmnote{ (#3)}}
\theoremstyle{exercise}
\newtheorem{exercise}{Exercise 6.B.}

\newtheoremstyle{solution}
    {}
    {}
    {}
    {}
    {\itshape\color{magenta}}
    {.}
    { }
    {\thmname{#1}\thmnote{ #3}}
\theoremstyle{solution}
\newtheorem*{solution}{Solution}

\Crefformat{exercise}{#2Exercise 6.B.#1#3}

\newcommand{\upd}{\,\text{d}}
\newcommand{\re}{\text{Re}\,}
\newcommand{\im}{\text{Im}\,}
\newcommand{\poly}{\mathcal{P}}
\newcommand{\lmap}{\mathcal{L}}
\newcommand{\mat}{\mathcal{M}}
\newcommand{\ts}{\textsuperscript}
\newcommand{\Span}{\text{span}}
\newcommand{\Null}{\text{null\,}}
\newcommand{\Range}{\text{range\,}}
\newcommand{\Rank}{\text{rank\,}}
\newcommand{\quand}{\quad \text{and} \quad}
\newcommand{\ipanon}{\langle \cdot, \cdot \rangle}
\newcommand{\normanon}{\lVert \, \cdot \, \rVert}
\newcommand{\setcomp}[1]{#1^{\mathsf{c}}}
\newcommand{\tpose}[1]{#1^{\text{t}}}
\newcommand{\N}{\mathbf{N}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\F}{\mathbf{F}}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}
% Swap the definition of \abs* and \norm*, so that \abs
% and \norm resizes the size of the brackets, and the 
% starred version does not.
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}

\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\makeatletter
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\DeclarePairedDelimiter\paren{(}{)}
\makeatletter
\let\oldparen\paren
\def\paren{\@ifstar{\oldparen}{\oldparen*}}
\makeatother

\DeclarePairedDelimiter\bkt{[}{]}
\makeatletter
\let\oldbkt\bkt
\def\bkt{\@ifstar{\oldbkt}{\oldbkt*}}
\makeatother

\DeclarePairedDelimiter\Set{\{}{\}}
\makeatletter
\let\oldSet\Set
\def\Set{\@ifstar{\oldSet}{\oldSet*}}
\makeatother

\DeclarePairedDelimiter\ip{\langle}{\rangle}
\makeatletter
\let\oldip\ip
\def\set{\@ifstar{\oldip}{\oldip*}}
\makeatother

\setlist[enumerate,1]{label={(\alph*)}}

\begin{document}

\section{Section 6.B Exercises}

Exercises with solutions from Section 6.B of \hyperlink{ladr}{[LADR]}.

\begin{exercise}
\label{ex:1}
    \begin{enumerate}
        \item Suppose \( \theta \in \R \). Show that \( ( \cos \theta, \sin \theta ), ( -\sin \theta, \cos \theta ) \) and \( ( \cos \theta, \sin \theta ), \) \( ( \sin \theta, -\cos \theta ) \) are orthonormal bases of \( \R^2 \).

        \item Show that each orthonormal basis of \( \R^2 \) is of the form given by one of the two possibilities of part (a).
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item Observe that
        \begin{gather*}
            \ip{ ( \cos \theta, \sin \theta ), ( \cos \theta, \sin \theta ) } = \ip{ ( -\sin \theta, \cos \theta ), ( -\sin \theta, \cos \theta ) } = \cos^2 \theta + \sin^2 \theta = 1, \\[2mm]
            \ip{ ( \cos \theta, \sin \theta ), ( -\sin \theta, \cos \theta ) } = \cos \theta \sin \theta - \cos \theta \sin \theta = 0.
        \end{gather*}
        Thus \( ( \cos \theta, \sin \theta ), ( -\sin \theta, \cos \theta ) \) is an orthonormal basis of \( \R^2 \), and we can similarly show that \( ( \cos \theta, \sin \theta ), \) \( ( \sin \theta, -\cos \theta ) \) is an orthonormal basis of \( \R^2 \).

        \item
        \begin{figure}[H]
            \centering
            \begin{tikzpicture}
                \draw[-latex] (0, -5) -- (0, 5);
                \draw[-latex] (-5, 0) -- (5, 0);

                \draw[gray] (0, 0) circle (4);
                \draw[-latex, ultra thick] (0, 0) -- ({4*cos(50)}, {4*sin(50)});
                \draw[-latex, thick] (0:1.4) arc (0:50:1.4);

                \draw[-latex, dashed, ultra thick] (0, 0) -- ({4*cos(-40)}, {4*sin(-40)});
                \draw[-latex, dashed, ultra thick] (0, 0) -- ({4*cos(140)}, {4*sin(140)});

                \node at ({0.8*cos(25)}, {0.8*sin(25)}) {\( \theta \)};
                \node at ({2*cos(60)}, {2*sin(60)}) {\( u \)};
                \node at ({2*cos(-30)}, {2*sin(-30)}) {\( v \)};
                \node at ({2*cos(130)}, {2*sin(130)}) {\( v \)};
            \end{tikzpicture}
            \caption{Orthonormal basis of \( \R^2 \)}
            \label{fig:1}
        \end{figure}
        Suppose \( u, v \) is an orthonormal basis of \( \R^2 \). Let \( \theta \) be the angle that \( u \) makes with the positive \( x \)-axis, as shown in \Cref{fig:1}. Note that \( \norm{u} = 1 \), so that \( u \) lies on the circle of radius 1 centered at the origin in \( \R^2 \). It follows that \( u = (\cos \theta, \sin \theta) \).
        
        Since \( u \) and \( v \) are orthogonal, plane geometry tells us that \( v \) either makes an angle of \( \theta + \tfrac{\pi}{2} \) or \( \theta - \tfrac{\pi}{2} \) with the positive \( x \)-axis, and since \( \norm{v} = 1 \) we know that \( v \) also lies on the circle of radius 1 centered at the origin in \( \R^2 \). It follows that
        \begin{multline*}
            v = \paren{ \cos \paren{ \theta + \tfrac{\pi}{2} }, \sin \paren{ \theta + \tfrac{\pi}{2} } } = ( -\sin \theta, \cos \theta ) \\[2mm]
            \text{or} \quad v = \paren{ \cos \paren{ \theta - \tfrac{\pi}{2} }, \sin \paren{ \theta - \tfrac{\pi}{2} } } = ( \sin \theta, -\cos \theta ).
        \end{multline*}
    \end{enumerate}
\end{solution}

\begin{exercise}
\label{ex:2}
    Suppose \( e_1, \ldots, e_m \) is an orthonormal list of vectors in \( V \). Let \( v \in V \). Prove that
    \[
        \norm{v}^2 = \abs{\ip{v, e_1}}^2 + \cdots + \abs{\ip{v, e_m}}^2
    \]
    if and only if \( v \in \Span(e_1, \ldots, e_m) \).
\end{exercise}

\begin{solution}
    Suppose \( v \in \Span(e_1, \ldots e_m) \). Note that \( e_1, \ldots, e_m \) is linearly independent (6.26) and hence is a basis of \( \Span(e_1, \ldots, e_m) \). It follows from 6.30 that
    \[
        \norm{v}^2 = \abs{\ip{v, e_1}}^2 + \cdots + \abs{\ip{v, e_m}}^2.
    \]
    Now suppose that \( \norm{v}^2 = \abs{\ip{v, e_1}}^2 + \cdots + \abs{\ip{v, e_m}}^2 \), let \( v' = \ip{v, e_1} e_1 + \cdots + \ip{v, e_m} e_m \), and note that \( \norm{v'}^2 = \norm{v}^2 \) by 6.30. Observe that
    \[
        \ip{v', v} = \ip*{ \sum_{j=1}^m \ip{v, e_j} e_j, v } = \sum_{j=1}^m \ip{ \ip{v, e_j} e_j, v } = \sum_{j=1}^m \abs{\ip{v, e_j}}^2 = \norm{v}^2.
    \]
    It follows that
    \[
        \norm{v' - v}^2 = \ip{v' - v, v' - v} = \ip{v', v'} + \ip{v, v} - 2 \text{Re}\,\ip{v', v} = 2 \norm{v}^2 - 2 \norm{v}^2 = 0,
    \]
    from which it follows that \( v = v' \in \Span(e_1, \ldots, e_m) \).
\end{solution}

\begin{exercise}
\label{ex:3}
    Suppose \( T \in \lmap(\R^3) \) has an upper-triangular matrix with respect to the basis \( (1, 0, 0), (1, 1, 1), (1, 1, 2) \). Find an orthonormal basis of \( \R^3 \) (use the usual inner product on \( \R^3 \)) with respect to which \( T \) has an upper-triangular matrix.
\end{exercise}

\begin{solution}
    Let \( u_1 = (1, 0, 0), u_2 = (1, 1, 1), \) and \( u_3 = (1, 1, 2) \). According to the proof of 6.37, if we apply the Gram-Schmidt procedure to the basis \( u_1, u_2, u_3 \), we will obtain an orthonormal basis of \( \R^3 \) with respect to which \( T \) has an upper-triangular matrix.

    Since \( \norm{u_1} = 1 \), we can take \( e_1 = u_1 \). We calculate
    \begin{multline*}
        e_2 = \frac{u_2 - \ip{u_2, e_1} e_1}{\norm{u_2 - \ip{u_2, e_1} e_1}} = \paren{ 0, \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}} } \\[2mm]
        \text{and} \quad e_3 = \frac{u_3 - \ip{u_3, e_2} e_2 - \ip{u_3, e_1} e_1}{\norm{u_3 - \ip{u_3, e_2} e_2 - \ip{u_3, e_1} e_1}} = \paren{ 0, -\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}} }.
    \end{multline*}
    Thus \( e_1, e_2, e_3 \) is the desired orthonormal basis.
\end{solution}

\begin{exercise}
\label{ex:4}
    Suppose \( n \) is a positive integer. Prove that
    \[
        \frac{1}{\sqrt{2 \pi}}, \frac{\cos x}{\sqrt{\pi}}, \frac{\cos 2x}{\sqrt{\pi}}, \ldots, \frac{\cos nx}{\sqrt{\pi}}, \frac{\sin x}{\sqrt{\pi}}, \frac{\sin 2x}{\sqrt{\pi}}, \ldots, \frac{\sin nx}{\sqrt{\pi}}
    \]
    is an orthonormal list of vectors in \( C[-\pi, \pi] \), the vector space of continuous real-valued functions on \( [-\pi, \pi] \) with inner product
    \[
        \ip{f, g} = \int_{-\pi}^{\pi} f(x) g(x) \, \upd x.
    \]
    [\textit{The orthonormal list above is often used for modeling periodic phenomena such as tides.}]
\end{exercise}

\begin{solution}
    We calculate, for \( 1 \leq j \leq n \),
    \begin{gather*}
        \ip*{\frac{1}{\sqrt{2 \pi}}, \frac{1}{\sqrt{2 \pi}}} = \int_{-\pi}^{\pi} \paren{\frac{1}{\sqrt{2 \pi}}}^2 \, \upd x = \frac{1}{2 \pi} \int_{-\pi}^{\pi} \upd x = 1; \\[3mm]
        \ip*{\frac{\cos jx}{\sqrt{\pi}}, \frac{\cos jx}{\sqrt{\pi}}} = \int_{-\pi}^{\pi} \paren{\frac{\cos jx}{\sqrt{\pi}}}^2 \, \upd x = \frac{2}{j \pi} \int_{0}^{j \pi} \cos^2 y \, \upd y = \frac{1}{j \pi} \bkt{y + \sin y \cos y}_{y = 0}^{y = j \pi} = 1; \\[3mm]
        \ip*{\frac{\sin jx}{\sqrt{\pi}}, \frac{\sin jx}{\sqrt{\pi}}} = \int_{-\pi}^{\pi} \paren{\frac{\sin jx}{\sqrt{\pi}}}^2 \, \upd x = \frac{2}{j \pi} \int_{0}^{j \pi} \sin^2 y \, \upd y = \frac{1}{j \pi} \bkt{y - \sin y \cos y}_{y = 0}^{y = j \pi} = 1; \\[3mm]
        \ip*{\frac{1}{\sqrt{2 \pi}}, \frac{\cos jx}{\sqrt{\pi}}} = \int_{-\pi}^{\pi} \frac{1}{\sqrt{2 \pi}} \frac{\cos jx}{\sqrt{\pi}} \, \upd x = \frac{\sqrt{2}}{j \pi} \int_{0}^{j \pi} \cos y \, \upd y = \frac{\sqrt{2}}{j \pi} \bkt{\sin y}_{y=0}^{y=j \pi} = 0; \\[3mm]
        \ip*{\frac{1}{\sqrt{2 \pi}}, \frac{\sin jx}{\sqrt{\pi}}} = \int_{-\pi}^{\pi} \frac{1}{\sqrt{2 \pi}} \frac{\sin jx}{\sqrt{\pi}} \, \upd x = 0;
    \end{gather*}
    where we have used that \( \sin jx \) is an odd function for the last equality. For \( 1 \leq j, k \leq n \) such that \( j \neq k \), we have
    \begin{multline*}
        \ip*{\frac{\cos jx}{\sqrt{\pi}}, \frac{\cos kx}{\sqrt{\pi}}} = \int_{-\pi}^{\pi} \frac{\cos jx}{\sqrt{\pi}} \frac{\cos kx}{\sqrt{\pi}} \, \upd x = \frac{1}{\pi} \int_0^{\pi} \cos((j - k)x) + \cos((j + k)x) \, \upd x \\[3mm]
        = \frac{1}{\pi} \bkt{\frac{\sin((j - k)x)}{j - k}}_{x=0}^{x=\pi} + \frac{1}{\pi} \bkt{\frac{\sin((j + k)x)}{j + k}}_{x=0}^{x=\pi} = 0;
    \end{multline*}
    \begin{multline*}
        \ip*{\frac{\sin jx}{\sqrt{\pi}}, \frac{\sin kx}{\sqrt{\pi}}} = \int_{-\pi}^{\pi} \frac{\sin jx}{\sqrt{\pi}} \frac{\sin kx}{\sqrt{\pi}} \, \upd x = \frac{1}{\pi} \int_0^{\pi} \cos((j - k)x) - \cos((j + k)x) \, \upd x \\[3mm]
        = \frac{1}{\pi} \bkt{\frac{\sin((j - k)x)}{j - k}}_{x=0}^{x=\pi} - \frac{1}{\pi} \bkt{\frac{\sin((j + k)x)}{j + k}}_{x=0}^{x=\pi} = 0.
    \end{multline*}
    Finally, for any \( 1 \leq j, k \leq n \), we have
    \[
        \ip*{\frac{\cos jx}{\sqrt{\pi}}, \frac{\sin kx}{\sqrt{\pi}}} = \int_{-\pi}^{\pi} \frac{\cos jx}{\sqrt{\pi}} \frac{\sin kx}{\sqrt{\pi}} \, \upd x = 0,
    \]
    where we have used that \( \cos jx \sin kx \) is an odd function for the last equality.
\end{solution}

\begin{exercise}
\label{ex:5}
    On \( \poly_2(\R) \), consider the inner product given by
    \[
        \ip{p, q} = \int_0^1 p(x) q(x) \, \upd x.
    \]
    Apply the Gram-Schmidt Procedure to the basis \( 1, x, x^2 \) to produce an orthonormal basis of \( \poly_2(\R) \).
\end{exercise}

\begin{solution}
    Since \( \int_0^1 \upd x = 1 \), we can take \( e_1 = 1 \). We calculate
    \[
        \ip{x, e_1} = \int_0^1 x \, \upd x = \frac{1}{2} \quand \norm{x - \ip{x, e_1}e_1} = \paren{ \int_0^1 \paren{x - \tfrac{1}{2}}^2 \, \upd x }^{1/2} = \frac{1}{2 \sqrt{3}}.
    \]
    and thus
    \[
        e_2 = \frac{x - \ip{x, e_1} e_1}{\norm{x - \ip{x, e_1} e_1}} = 2 \sqrt{3} \paren{x - \tfrac{1}{2}}.
    \]
    Similarly,
    \begin{multline*}
        \ip{x^2, e_1} = \int_0^1 x^2 \, \upd x = \frac{1}{3}, \quad \ip{x^2, e_2} = 2 \sqrt{3} \int_0^1 x^2 \paren{x - \tfrac{1}{2}} \, \upd x = \frac{1}{2 \sqrt{3}}, \\[3mm]
        \text{and} \quad \norm{x^2 - \ip{x^2, e_2} e_2 - \ip{x^2, e_1} e_1} = \paren{ \int_0^1 \paren{ x^2 - x + \tfrac{1}{6} }^2 \, \upd x }^{1/2} = \frac{1}{6 \sqrt{5}}.
    \end{multline*}
    This gives us
    \[
        e_3 = \frac{x^2 - \ip{x^2, e_2} e_2 - \ip{x^2, e_1} e_1}{\norm{x^2 - \ip{x^2, e_2} e_2 - \ip{x^2, e_1} e_1}} = 6 \sqrt{5} \paren{ x^2 - x + \tfrac{1}{6} }.
    \]
    Thus
    \[
        e_1 = 1, \quad e_2 = 2 \sqrt{3} \paren{ x - \tfrac{1}{2} }, \quad e_3 = 6 \sqrt{5} \paren{ x^2 - x + \tfrac{1}{6} }
    \]
    is an orthonormal basis of \( \poly_2(\R) \) with respect to the given inner product.
\end{solution}

\begin{exercise}
\label{ex:6}
    Find an orthonormal basis of \( \poly_2(\R) \) (with inner product as in Exercise 5) such that the differentiation operator (the operator that takes \( p \) to \( p' \)) on \( \poly_2(\R) \) has an upper-triangular matrix with respect to this basis.
\end{exercise}

\begin{solution}
    Observe that the differentiation operator has the upper-triangular matrix
    \[
        \begin{pmatrix}
            0 & 1 & 0 \\
            0 & 0 & 2 \\
            0 & 0 & 0
        \end{pmatrix}
    \]
    with respect to the basis \( 1, x, x^2 \). As the proof of 6.37 shows, if we apply the Gram-Schmidt procedure to the basis \( 1, x, x^2 \) we will obtain an orthonormal basis with respect to which the differentiation operator has an upper-triangular matrix. This orthonormal basis is precisely the one obtained in \Cref{ex:5}.
\end{solution}

\begin{exercise}
\label{ex:7}
    Find a polynomial \( q \in \poly_2(\R) \) such that
    \[
        p \paren{ \tfrac{1}{2} } = \int_0^1 p(x) q(x) \, \upd x
    \]
    for every \( p \in \poly_2(\R) \).
\end{exercise}

\begin{solution}
    Define the linear functional \( \varphi : \poly_2(\R) \to \R \) by \( \varphi(p) = p \paren{ \tfrac{1}{2} } \). As 6.43 (in the proof of the Riesz Representation Theorem, 6.42) shows, if we take
    \[
        q(x) = \varphi(e_1) e_1(x) + \varphi(e_2) e_2(x) + \varphi(e_3) e_3(x) = -15 x^2 + 15 x - \tfrac{3}{2},
    \]
    where \( e_1, e_2, e_3 \) is the orthonormal basis of \( \poly_2(\R) \) found in \Cref{ex:5}, then
    \[
        \varphi(p) = p \paren{\tfrac{1}{2}} = \ip{p, q} = \int_0^1 p(x) q(x) \, \upd x
    \]
    for every \( p \in \poly_2(\R) \).
\end{solution}

\begin{exercise}
\label{ex:8}
    Find a polynomial \( q \in \poly_2(\R) \) such that
    \[
        \int_0^1 p(x) (\cos \pi x) \, \upd x = \int_0^1 p(x) q(x) \, \upd x  
    \]
    for every \( p \in \poly_2(\R) \).
\end{exercise}

\begin{solution}
    Define the linear functional \( \varphi : \poly_2(\R) \to \R \) by \( \varphi(p) = \int_0^1 p(x) (\cos \pi x) \, \upd x \). As 6.43 (in the proof of the Riesz Representation Theorem, 6.42) shows, if we take
    \[
        q(x) = \varphi(e_1) e_1(x) + \varphi(e_2) e_2(x) + \varphi(e_3) e_3(x) = - \frac{24}{\pi^2} \paren{ x - \tfrac{1}{2} },
    \]
    where \( e_1, e_2, e_3 \) is the orthonormal basis of \( \poly_2(\R) \) found in \Cref{ex:5}, then
    \[
        \varphi(p) = \int_0^1 p(x) (\cos \pi x) \, \upd x = \ip{p, q} = \int_0^1 p(x) q(x) \, \upd x
    \]
    for every \( p \in \poly_2(\R) \).
\end{solution}

\begin{exercise}
\label{ex:9}
    What happens if the Gram-Schmidt Procedure is applied to a list of vectors that is not linearly independent?
\end{exercise}

\begin{solution}
    Division by zero will occur. This is clear if the list of vectors contains the zero vector, but the problem arises even if this is not the case. For example, if \( v \) is non-zero, then consider applying the Gram-Schmidt procedure to the linearly dependent list \( v, 2v \). We first set \( e_1 = v / \norm{v} \), and then run into a problem:
    \[
        2v - \ip{2v, e_1} e_1 = 2v - 2 \frac{\ip{v, v}}{\norm{v}^2} v = 2v - 2v = 0.
    \]
\end{solution}

\begin{exercise}
\label{ex:10}
    Suppose \( V \) is a real inner product space and \( v_1, \ldots, v_m \) is a linearly independent list of vectors in \( V \). Prove that there exist exactly \( 2^m \) orthonormal lists \( e_1, \ldots, e_m \) of vectors in \( V \) such that
    \[
        \Span(v_1, \ldots v_j) = \Span(e_1, \ldots, e_j)
    \]
    for all \( j \in \{ 1, \ldots, m \} \).
\end{exercise}

\begin{solution}
    Apply the Gram-Schmidt procedure (6.31) to the list \( v_1, \ldots, v_m \) to obtain an orthonormal list of vectors \( e_1, \ldots, e_m \) satisfying
    \[
        \Span(v_1, \ldots v_j) = \Span(e_1, \ldots, e_j)
    \]
    for all \( j \in \{ 1, \ldots, m \} \). Let \( \{ -1, 1 \}^m \) be the collection of functions \( f : \{ 1, \ldots, m \} \to \{ -1, 1 \} \). For \( f \in \{ -1, 1 \}^m \), it is straightforward to verify that the list \( f(1) e_1, \ldots, f(m) e_m \) is orthonormal and satisfies
    \[
        \Span(v_1, \ldots v_j) = \Span(f(1) e_1, \ldots, f(j) e_j)
    \]
    for all \( j \in \{ 1, \ldots, m \} \).

    Suppose that \( u_1, \ldots, u_m \) is an orthonormal list of vectors such that
    \[
        \Span(v_1, \ldots v_j) = \Span(e_1, \ldots, e_j) = \Span(u_1, \ldots, u_j)
    \]
    for all \( j \in \{ 1, \ldots, m \} \). In particular, we have \( \Span(e_1) = \Span(u_1) \), from which it follows that \( u_1 = \lambda e_1 \). Since \( \norm{u_1} = \norm{e_1} = 1 \), we see that \( \lambda = \pm 1 \); set \( f(1) = \lambda \). Since \( \Span(e_1, e_2) = \Span(u_1, u_2) \), we must have \( u_2 \in \Span(e_1, e_2) \). Since \( e_1, e_2 \) is an orthonormal basis of \( \Span(e_1, e_2) \), 6.30 implies that \( u_2 = \ip{u_2, e_1} e_1 + \ip{u_2, e_2} e_2 \). Note that the orthonormality of the list \( u_1, u_2 \) implies that
    \[
        0 = \ip{u_1, u_2} = \ip{f(1) e_1, u_2} = f(1) \ip{e_1, u_2},
    \]
    which implies that \( \ip{e_1, u_2} = 0 \) since \( f(1) = \pm 1 \). Thus \( u_2 = \lambda e_2 \) for some \( \lambda \in \R \); since \( \norm{u_2} = \norm{e_2} = 1 \), we again see that \( \lambda = \pm 1 \) and we can set \( f(2) = \lambda \).

    We continue in this manner, obtaining an \( f \in \{ -1, 1 \}^m \) such that \( u_j = f(j) e_j \) for each \( 1 \leq j \leq m \). It follows that the orthonormal lists \( u_1, \ldots, u_m \) satisfying
    \[
        \Span(v_1, \ldots v_j) = \Span(e_1, \ldots, e_j) = \Span(u_1, \ldots, u_j)
    \]
    for all \( j \in \{ 1, \ldots, m \} \) are precisely those of the form \( f(1) e_1, \ldots, f(m) e_m \) for \( f \in \{ -1, 1 \}^m \). Since there are \( 2^m \) such functions, and distinct elements of \( \{ -1, 1 \}^m \) give rise to distinct orthonormal lists, we see that there are exactly \( 2^m \) orthonormal lists with the desired property.
\end{solution}

\begin{exercise}
\label{ex:11}
    Suppose \( \ipanon_1 \) and \( \ipanon_2 \) are inner products on \( V \) such that \( \ip{v, w}_1 = 0 \) if and only if \( \ip{v, w}_2 = 0 \). Prove that there is a positive number \( c \) such that \( \ip{v, w}_1 = c \ip{v, w}_2 \) for every \( v, w \in V \).
\end{exercise}

\begin{solution}
    If \( V = \{ 0 \} \) then we may take any \( c > 0 \) we like, since the only inner product on \( V \) is the map \( (0, 0) \mapsto 0 \). Suppose therefore that \( V \neq \{ 0 \} \) and for each non-zero \( w \in V \) define
    \[
        c_w = \frac{\ip{w, w}_1}{\ip{w, w}_2} > 0.
    \]
    Suppose \( v, w \in V \) are non-zero. By orthogonal decomposition (6.14), we have
    \[
        \ip*{ v - \frac{\ip{v, w}_2}{\ip{w, w}_2} w, w }_2 = 0.
    \]
    Our assumption is that orthogonality with respect to \( \ipanon_2 \) is equivalent to orthogonality with respect to \( \ipanon_1 \) and thus
    \[
        \ip*{ v - \frac{\ip{v, w}_2}{\ip{w, w}_2} w, w }_1 = 0 \quad \iff \quad \ip{v, w}_1 - \frac{\ip{v, w}_2}{\ip{w, w}_2} \ip{w, w}_1 = 0 \quad \iff \quad \ip{v, w}_1 = c_w \ip{v, w}_2.
    \]
    Reversing the roles of \( v \) and \( w \) gives \( \ip{w, v}_1 = c_v \ip{w, v}_2 \) and combining this with conjugate symmetry gives us
    \[
        c_w \ip{v, w}_2 = \ip{v, w}_1 = \overline{\ip{w, v}_1} = \overline{c_v \ip{w, v}_2} = c_v \ip{v, w}_2.
    \]
    Let us summarize our findings:
    \begin{gather*}
        \text{for all non-zero } v, w \in V, \qquad \ip{v, w}_1 = c_w \ip{v, w}_2, \tag{1} \\[3mm]
        \text{for all non-zero } v, w \in V, \qquad c_w \ip{v, w}_2 = c_v \ip{v, w}_2. \tag{2}
    \end{gather*}
    Now, given non-zero \( v, w \in V \), there exists a non-zero \( u \in V \) such that \( \ip{u, v}_2 \neq 0 \) and \( \ip{w, u}_2 \neq 0 \); if \( \ip{v, w}_2 \neq 0 \) then take \( u = v \) and if \( \ip{v, w}_2 = 0 \) then take \( u = v + w \) (this is a special case of \Cref{ex:13}). Using (2), we have
    \[
        c_v \ip{u, v}_2 = c_u \ip{u, v}_2 \quand c_u \ip{w, u}_2 = c_w \ip{w, u}_2.
    \]
    Since \( \ip{u, v}_2 \neq 0 \) and \( \ip{w, u}_2 \neq 0 \), these two equations imply that \( c_v = c_u = c_w \). If we denote this common value by \( c \) (noting that \( c > 0 \)), then we have shown that \( c_w = c \) for all non-zero \( w \in V \). It follows from (1) that \( \ip{v, w}_1 = c \ip{v, w}_2 \) for all non-zero \( v, w \in V \); evidently this equation also holds if either \( v = 0 \) or \( w = 0 \) and thus \( \ip{v, w}_1 = c \ip{v, w}_2 \) for all \( v, w \in V \).
\end{solution}

\begin{exercise}
\label{ex:12}
    Suppose \( V \) is finite-dimensional and \( \ipanon_1, \ipanon_2 \) are inner products on \( V \) with corresponding norms \( \normanon_1 \) and \( \normanon_2 \). Prove that there exists a positive number \( c \) such that
    \[
        \norm{v}_1 \leq c \norm{v}_2  
    \]
    for every \( v \in V \).
\end{exercise}

\begin{solution}
    While this follows from the fact that any two norms on a finite-dimensional vector space are equivalent, which we proved in the solution to \href{https://lew98.github.io/Mathematics/LADR_Section_6_A_Exercises.pdf}{Exercise 6.A.29}, there is an easier proof for the special case where the two norms both arise from inner products.

    By 6.34, there exists an orthonormal basis \( e_1, \ldots, e_n \) of \( V \) with respect to \( \ipanon_2 \). Let \( v \in V \) be given, so that \( v = a_1 e_1 + \cdots + a_n e_n \) for some scalars \( a_1, \ldots, a_n \). Observe that
    \[
        \abs{a_1} + \cdots + \abs{a_n} \leq n \max \{ \abs{a_1}, \ldots, \abs{a_n} \} \leq n \sqrt{\abs{a_1}^2 + \cdots + \abs{a_n}^2} = n \norm{v}_2, \tag{1}
    \]
    where the last equality follows from 6.25. If we let \( M = \max \{ \norm{e_1}_1, \ldots, \norm{e_n}_1 \} \), which is positive since each \( e_j \neq 0 \), then it follows from (1) and the triangle inequality that
    \[
        \norm{v}_1 \leq \abs{a_1} \norm{e_1}_1 + \cdots + \abs{a_n} \norm{e_n}_1 \leq M \paren{\abs{a_1} + \cdots + \abs{a_n}} \leq nM \norm{v}_2.
    \]
    Thus the desired positive constant is \( c = nM \).
\end{solution}

\begin{exercise}
\label{ex:13}
    Suppose \( v_1, \ldots, v_m \) is a linearly independent list in \( V \). Show that there exists \( w \in V \) such that \( \ip{w, v_j} > 0 \) for all \( j \in \{ 1, \ldots, m \} \).
\end{exercise}

\begin{solution}[1]
    Let \( U = \Span(v_1, \ldots, v_m) \) and define a linear functional \( \varphi : U \to \F \) by
    \[
        \varphi(v) = a_1 + \cdots + a_m,
    \]
    where \( a_1, \ldots, a_m \) are the unique scalars satisfying \( v = a_1 v_1 + \cdots a_m v_m \). Since \( U \) is finite-dimensional, we may appeal to the Riesz representation theorem (6.42) to obtain a unique \( w \in U \) such that \( \varphi(v) = \ip{v, w} \) for all \( v \in U \). In particular, for any \( 1 \leq j \leq m \), we have
    \[
        1 = \varphi(v_j) = \ip{v_j, w} \quad \implies \quad \ip{w, v_j} > 0.
    \]
\end{solution}

\begin{solution}[2]
    Let \( U = \Span(v_1, \ldots, v_m) \) and note that \( \dim U = m \) since \( v_1, \ldots, v_m \) is a basis of \( U \). Define a linear map \( T : U \to \F^m \) by
    \[
        Tu = \paren{ \ip{u, v_1}, \ldots, \ip{u, v_m} }.
    \]
    Suppose that \( u \in U \) is such that \( Tu = 0 \), which is the case if and only if \( \ip{u, v_j} = 0 \) for all \( 1 \leq j \leq m \). Since \( u \in U \), we have \( u = a_1 v_1 + \cdots + a_m v_m \) for some scalars \( a_1, \ldots, a_m \). Observe that
    \[
        \ip{u, u} = \ip{u, a_1 v_1 + \cdots + a_m v_m} = \overline{a_1} \ip{u, v_1} + \cdots + \overline{a_m} \ip{u, v_m} = 0.
    \]
    It follows that \( u = 0 \) and hence that \( T \) is injective. Since \( \dim U = \dim \F^m = m \), rank-nullity (3.22) implies that \( T \) is surjective. Consequently, there exists some \( w \in U \) such that
    \[
        Tw = \paren{ \ip{w, v_1}, \ldots, \ip{w, v_m} } = (1, \ldots, 1).
    \]
\end{solution}

\begin{exercise}
\label{ex:14}
    Suppose \( e_1, \ldots, e_n \) is an orthonormal basis of \( V \) and \( v_1, \ldots, v_n \) are vectors in \( V \) such that
    \[
        \norm{e_j - v_j} < \frac{1}{\sqrt{n}}
    \]
    for each \( j \). Prove that \( v_1, \ldots, v_n \) is a basis of \( V \).
\end{exercise}

\begin{solution}
    It will suffice to show that \( v_1, \ldots, v_n \) is linearly independent, so suppose that \( \sum_{j=1}^n a_j v_j = 0 \). Then
    \begin{align*}
        \textstyle\sum_{j=1}^n \abs{a_j}^2 &= \norm{ \textstyle\sum_{j=1}^n a_j e_j }^2 \tag{6.25} \\[2mm]
        &= \norm{ \textstyle\sum_{j=1}^n a_j(e_j - v_j) }^2 \tag{\( \textstyle\sum_{j=1}^n a_j v_j = 0 \)} \\[2mm]
        &\leq \paren{ \textstyle\sum_{j=1}^n \abs{a_j} \norm{e_j - v_j} }^2 \tag{triangle inequality} \\[2mm]
        &\leq \paren{ \textstyle\sum_{j=1}^n \abs{a_j}^2 } \paren{ \textstyle\sum_{j=1}^n \norm{e_j - v_j}^2 }. \tag{Cauchy-Schwarz inequality}
    \end{align*}
    By assumption we have \( \sum_{j=1}^n \norm{e_j - v_j}^2 < 1 \); it follows that \( \sum_{j=1}^n \abs{a_j}^2 = 0 \), which is the case if and only if each \( a_j = 0 \).
\end{solution}

\begin{exercise}
\label{ex:15}
    Suppose \( C_{\R}([-1, 1]) \) is the vector space of continuous real-valued functions on the interval \( [-1, 1] \) with inner product given by
    \[
        \ip{f, g} = \int_{-1}^1 f(x) g(x) \upd x
    \]
    for \( f, g \in C_{\R}([-1, 1]) \). Let \( \varphi \) be the linear functional on \( C_{\R}([-1, 1]) \) defined by \( \varphi(f) = f(0) \). Show that there does not exist \( g \in C_{\R}([-1, 1]) \) such that
    \[
        \varphi(f) = \ip{f, g}
    \]
    for every \( f \in C_{\R}([-1, 1]) \).

    \noindent [\textit{The exercise above shows that the Riesz Representation Theorem (6.42) does not hold on infinite-dimensional vector spaces without additional hypotheses on \( V \) and \( \varphi \).}]
\end{exercise}

\begin{solution}
    Suppose such a \( g \) exists and define \( h \in C_{\R}([-1, 1]) \) by \( h(x) = x^2 g(x) \). Then
    \[
        0 = h(0) = \int_{-1}^1 [x g(x)]^2 \upd x,
    \]
    which is the case if and only if \( x g(x) = 0 \) for all \( x \in [-1, 1] \) (as the integrand is a non-negative and continuous function). This implies that \( g(x) = 0 \) for all \( x \in [-1, 1] \), except possibly at \( x = 0 \); but \( g \) is continuous, so in fact \( g \) must satisfy \( g(0) = 0 \) also. This implies that
    \[
        f(0) = \int_{-1}^1 f(x) g(x) \upd x = 0
    \]
    for all \( f \in C_{\R}([-1, 1]) \), which is clearly not true. We may conclude that no such \( g \) exists.
\end{solution}

\begin{exercise}
\label{ex:16}
    Suppose \( \F = \C, V \) is finite-dimensional, \( T \in \lmap(V) \), all the eigenvalues of \( T \) have absolute value less than 1, and \( \epsilon > 0 \). Prove that there exists a positive integer \( m \) such that \( \norm{T^m v} \leq \epsilon \norm{v} \) for every \( v \in V \).
\end{exercise}

\begin{solution}
    We will first translate the problem into a statement involving column vectors in \( \C^{n,1} \) and matrices in \( \C^{n,n} \), where \( n = \dim V \). For the avoidance of confusion, we shall use the following notation.
    \begin{itemize}
        \item \( \ipanon_V : V \times V \to \C \). This is the given inner product on \( V \).
        
        \item \( \normanon_V : V \to [0, \infty) \). This is the norm on \( V \) arising from \( \ipanon_V \), i.e.\
        \[
            \norm{v}_V = \sqrt{\ip{v, v}}.
        \]

        \item \( \ipanon : \C^{n,1} \times \C^{n,1} \to \C \). This is the Euclidean inner product on \( \C^{n,1} \), i.e.\
        \[
            \ip{x, y} = \sum_{j=1}^n x_j \overline{y_n},
        \]
        where \( x = \tpose{\begin{pmatrix} x_1 & \cdots & x_n \end{pmatrix}} \) and \( y = \tpose{\begin{pmatrix} y_1 & \cdots & y_n \end{pmatrix}} \).

        \item \( \normanon : \C^{n,1} \to [0, \infty) \). This is the Euclidean norm on \( \C^{n,1} \) arising from \( \ipanon \), i.e.\
        \[
            \norm{x} = \sqrt{\ip{x, x}} = \paren{ \sum_{j=1}^n \abs{x_j}^2 }^{1/2},
        \]
        where \( x = \tpose{\begin{pmatrix} x_1 & \cdots & x_n \end{pmatrix}} \).
    \end{itemize}
    For this exercise, we will use the following useful lemmas about matrices.

    \vspace{2mm}

    \noindent \textbf{Lemma 1.} If \( A \in \C^{n,n} \), then there exists a non-negative constant \( C \in \R \), depending only on \( A \) and \( n \), such that
    \[
        \norm{Ax} \leq C \norm{x} \text{ for all } x \in \C^{n,1}.
    \]

    \vspace{2mm}

    \noindent \textit{Proof.} Suppose that
    \[
        A = \begin{pmatrix}
            A_{1,1} & A_{1,2} & \cdots & A_{1,n} \\
            A_{2,1} & A_{2,2} & \cdots & A_{2,n} \\
            \vdots & \vdots & \ddots & \vdots \\
            A_{n,1} & A_{n,2} & \cdots & A_{n,n}
        \end{pmatrix}
        \quand
        x = \begin{pmatrix}
            x_1 \\
            x_2 \\
            \vdots \\
            x_n
        \end{pmatrix},
        \quad \text{so that} \quad
        Ax = \begin{pmatrix}
            \sum_{k=1}^n A_{1,k} x_k \\
            \sum_{k=1}^n A_{2,k} x_k \\
            \vdots \\
            \sum_{k=1}^n A_{n,k} x_k 
        \end{pmatrix}.
    \]
    Then
    \begin{align*}
        \norm{Ax}^2 &= \sum_{j=1}^n \abs{ \sum_{k=1}^n A_{j,k} x_k }^2 \\[2mm]
        &\leq \sum_{j=1}^n \paren{ \sum_{k=1}^n \abs{A_{j,k}} \abs{x_k} }^2 \tag{triangle inequality} \\[2mm]
        &\leq \sum_{j=1}^n \paren{ \sum_{k=1}^n \abs{A_{j,k}}^2 } \paren{ \sum_{k=1}^n \abs{x_k}^2 } \tag{Cauchy-Schwarz inequality} \\[2mm]
        &= \paren{ \sum_{j=1}^n \sum_{k=1}^n \abs{A_{j,k}}^2 } \norm{x}^2.
    \end{align*}
    It follows that \( \norm{Ax} \leq C \norm{x} \), where
    \[
        C := \paren{ \sum_{j=1}^n \sum_{k=1}^n \abs{A_{j,k}}^2 }^{1/2}
    \]
    is real, non-negative, and depends only on \( A \) and \( n \). \qed

    \vspace{2mm}

    \noindent In what follows, by a strictly upper-triangular matrix we mean a matrix that is upper-triangular and whose diagonal entries are also zero.

    \vspace{2mm}

    \noindent \textbf{Lemma 2.} Suppose that \( A \in \C^{n,n} \) is diagonal and \( B \in \C^{n,n} \) is strictly upper-triangular. Then \( AB \) and \( BA \) are both strictly upper-triangular.

    \vspace{2mm}

    \noindent \textit{Proof.} To show that \( AB \) is strictly upper-triangular, we need to show that \( (AB)_{i,k} = 0 \) for \( i \geq k \). Indeed,
    \[
        (AB)_{i,k} = \sum_{j=1}^n A_{i,j} B_{j,k} = A_{i,i} B_{i,k} = 0;
    \]
    the second equality follows since \( A \) is diagonal and the last equality follows since \( B \) is strictly upper-triangular. The proof that \( BA \) is strictly upper-triangular is similar. \qed

    \vspace{2mm}

    \noindent \textbf{Lemma 3.} Suppose that \( A \in \C^{n,n} \) is strictly upper-triangular. Then \( A^n = 0 \).

    \vspace{2mm}

    \noindent \textit{Proof.} Let us think of \( A \) as a linear map \( \C^{n,1} \to \C^{n,1} \); our goal is to show that \( A^n \) is the zero map.

    Let \( e_1, \ldots, e_n \) be the standard basis of \( \C^{n,1} \). Define \( U_0 = \{ 0 \} \) and \( U_j = \Span(e_1, \ldots, e_j) \) for \( j = 1, \ldots, n \); note that \( U_n = \C^{n,1} \). The fact that \( A \) is upper-triangular means that \( A(U_j) \subseteq U_{j-1} \) for each \( j \) and so
    \[
        A(U_n) \subseteq U_{n-1} \quad \implies \quad A^2(U_n) \subseteq A(U_{n-1}) \subseteq U_{n-2}.
    \]
    Continuing in this manner, we see that \( A^n (U_n) \subseteq U_0 = \{ 0 \} \), i.e.\ \( A^n (\C^{n,1}) = \{ 0 \} \). It follows that \( A^n \) is the zero map. \qed

    Let us return to the exercise. Schur's theorem (6.38) implies that there is an orthonormal (with respect to \( \ipanon_V \)) basis \( e_1, \ldots, e_n \) of \( V \) such that the matrix \( U := \mat(T, (e_1, \ldots, e_n)) \in \C^{n,n} \) is upper-triangular. Given a vector \( v = x_1 e_1 + \cdots + x_n e_n \in V \), let \( x = \mat(v) = \tpose{\begin{pmatrix} x_1 & \cdots & x_n \end{pmatrix}} \in \C^{n,1} \) (recall the matrix of a vector from 3.62). Observe that
    \[
        \norm{v}_V = \paren{ \sum_{j=1}^n \abs{x_j}^2 }^{1/2} = \norm{x},
    \]
    where the first equality follows from 6.25. Furthermore, for any positive integer \( m \) we have \( U^m = \mat(T^m) \) by 3.43; combining this with 3.65 we see that \( \mat(T^m v) = U^m x \). Consequently, the inequality \( \norm{T^m v}_V \leq \epsilon \norm{v}_V \) is equivalent to \( \norm{U^m x} \leq \epsilon \norm{x} \). Given this, to prove that there exists a positive integer \( m \) such that
    \[
        \norm{T^m v}_V \leq \epsilon \norm{v}_V \text{ for all } v \in V,
    \]
    it will suffice to show that there exists a positive integer \( m \) such that
    \[
        \norm{U^m x} \leq \epsilon \norm{x} \text{ for all } x \in \C^{n,1}.
    \]
    Let \( D \) be the diagonal matrix whose diagonal entries are exactly those of \( U \), and let \( N \) be the strictly upper-triangular matrix whose entries above the diagonal are exactly those of \( U \), so that \( U = D + N \). Let \( C \) be the non-negative real constant obtained from Lemma 1 for the matrix \( N \) and let
    \[
        \rho = \max \{ \abs{D_{1,1}}, \ldots, \abs{D_{n,n}} \} = \max \{ \abs{U_{1,1}}, \ldots, \abs{U_{n,n}} \};  
    \]
    note that \( 0 \leq \rho < 1 \) since the diagonal elements of \( D \) and \( U \) are precisely the eigenvalues of \( T \). Note further that
    \[
        \norm{Dx} = \paren{ \abs{D_{1,1}}^2 \abs{x_1}^2 + \cdots + \abs{D_{n,n}}^2 \abs{x_n}^2 }^{1/2} \leq \rho \norm{x} \tag{1}
    \]
    for any \( x \in \C^{n,1} \).
    
    For a positive integer \( m \), let \( H_m \) be the collection of all matrix products of length \( m \) formed from the matrices \( D \) and \( N \). For example,
    \begin{multline*}
        H_1 = \{ D, N \}, \quad H_2 = \{ DD, DN, ND, NN \}, \\
        H_3 = \{ DDD, DDN, DND, DNN, NDD, NDN, NND, NNN \}.
    \end{multline*}
    It can then be shown by induction that
    \[
        (D + N)^m = \sum_{A \in H_m} A.
    \]
    Observe that if \( m \geq n, A \in H_m \), and \( A \) contains \( n \) or more \( N \)'s, then by Lemma 2 the matrix product \( A \) must contain at least \( n \) consecutive strictly upper-triangular matrices; it follows from Lemma 3 that \( A = 0 \). Given this, we are only interested in matrix products \( A \in H_m \) which contain \( n - 1 \) or less \( N \)'s; let \( H_m' \) be the collection of all such matrix products, so that
    \[
        (D + N)^m = \sum_{A \in H_m'} A.
    \]
    Suppose \( A \in H_m' \) is a product of \( k \) \( N \)'s and \( m - k \) \( D \)'s in some order, where \( 0 \leq k \leq n - 1 \); note that there are \( \binom{m}{k} \) such products (think of \( m \) \( D \)'s in a row; we have to replace \( k \) of them with \( N \)'s). If \( x \in \C^{n,1} \), then it follows from Lemma 1 and inequality (1) that
    \[
        \norm{Ax} \leq \rho^{m-k} C^k \norm{x}.
    \]
    This is best demonstrated by example:
    \[
        \norm{DNDx} \leq \rho \norm{NDx} \leq \rho C \norm{Dx} \leq \rho^2 C \norm{x}.
    \]
    It now follows that for any \( x \in \C^{n,1} \) we have
    \[
        \norm{U^m x} = \norm{(D + N)^m x} = \norm{ \textstyle\sum_{A \in H_m'} Ax} \leq \sum_{A \in H_m'} \norm{Ax} \leq \sum_{k=0}^{n-1} \binom{m}{k} \rho^{m-k} C^k \norm{x}.
    \]
    For \( 0 \leq k \leq n - 1 \), we have
    \[
        \binom{m}{k} = \frac{m!}{k! (m-k)!} \leq \frac{m!}{(m-k)!} = m (m-1) \cdots (m - k + 1) \leq m^k \leq m^{n-1}.
    \]
    Thus, letting \( \mu = \max \{ 1, C, C^2, \ldots, C^{n-1} \} \), we have
    \[
        \norm{U^m x} \leq \mu n m^{n-1} \rho^{m - n + 1} \norm{x}
    \]
    for any \( x \in \C^{n,1} \). Since \( 0 \leq \rho < 1 \), L'Hôpital's rule shows that \( \lim_{m \to \infty} \mu n m^{n-1} \rho^{m - n + 1} = 0 \). Consequently, we may choose a positive integer \( m \) so that
    \[
        \norm{U^m x} \leq \epsilon \norm{x}
    \]
    for every \( x \in \C^{n,1} \).
\end{solution}

\begin{exercise}
\label{ex:17}
    For \( u \in V \), let \( \Phi u \) denote the linear functional on \( V \) defined by
    \[
        (\Phi u)(v) = \ip{v, u}
    \]
    for \( v \in V \).
    \begin{enumerate}
        \item Show that if \( \F = \R \), then \( \Phi \) is a linear map from \( V \) to \( V' \). (Recall from Section 3.F that \( V' = \lmap(V, \F) \) and that \( V' \) is called the dual space of \( V \).)

        \item Show that if \( \F = \C \) and \( V \neq \{ 0 \} \), then \( \Phi \) is not a linear map.

        \item Show that \( \Phi \) is injective.

        \item Suppose \( \F = \R \) and \( V \) is finite-dimensional. Use parts (a) and (c) and a dimension-counting argument (but without using 6.42) to show that \( \Phi \) is an isomorphism from \( V \) onto \( V' \).
    \end{enumerate}

    \noindent [\textit{Part (d) gives an alternative proof of the Riesz Representation Theorem (6.42) when \( \F = \R \). Part (d) also gives a natural isomorphism (meaning that it does not depend on a choice of basis) from a finite-dimensional real inner product space onto its dual space.}]
\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item This follows from 6.7 (d) and (e), noting that \( \overline{\lambda} = \lambda \) if \( \lambda \in \R \).

        \item Since \( V \neq \{ 0 \} \), there is some non-zero \( u \in V \). Observe that
        \[
            (\Phi iu)(u) = \ip{u, iu} = -i \ip{u, u} \quand (i \Phi u)(u) = i \ip{u, u}.
        \]
        These are not equal since \( \ip{u, u} \neq 0 \). It follows that \( \Phi iu \neq i \Phi u \) and hence that \( \Phi \) is not a linear map.

        \item Despite part (b), we can still use the equivalence
        \[
            \Phi \text{ is injective} \quad \iff \quad \Null \Phi = \{ 0 \}
        \]
        for either \( \F = \R \) or \( \F = \C \), since the proof of this uses only the additivity of \( \Phi \), which follows from 6.7 (d). Suppose therefore that \( u \in V \) is such that \( \Phi u = 0 \), which is to say that \( \ip{v, u} = 0 \) for all \( v \in V \). In particular we have \( \ip{u, u} = 0 \), which is the case if and only if \( u = 0 \). It follows that \( \Null \Phi = \{ 0 \} \) and hence that \( \Phi \) is injective.

        \item Parts (a) and (c) imply that \( \Phi \) is an injective linear map from \( V \) to \( V' \). Since \( \dim V = \dim V' \) (3.95), the rank-nullity theorem (3.22) implies that \( \Phi \) must be surjective. Thus \( \Phi \) is an isomorphism.
    \end{enumerate}
\end{solution}

\noindent \hrulefill

\noindent \hypertarget{ladr}{\textcolor{blue}{[LADR]} Axler, S. (2015) \textit{Linear Algebra Done Right.} 3\ts{rd} edition.}

\end{document}