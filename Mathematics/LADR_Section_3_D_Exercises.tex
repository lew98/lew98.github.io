\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{changepage}
\usepackage{lipsum}
\usepackage{tikz}
\usetikzlibrary{matrix}
\usepackage{tikz-cd}
\usepackage[nameinlink]{cleveref}
\geometry{
headheight=15pt,
left=60pt,
right=60pt
}
\setlength{\emergencystretch}{20pt}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{}
\chead{Section 3.D Exercises}
\rhead{\thepage}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue
}

\theoremstyle{definition}
\newtheorem*{remark}{Remark}

\newtheoremstyle{exercise}
    {}
    {}
    {}
    {}
    {\bfseries}
    {.}
    { }
    {\thmname{#1}\thmnumber{#2}\thmnote{ (#3)}}
\theoremstyle{exercise}
\newtheorem{exercise}{Exercise 3.D.}

\newtheoremstyle{solution}
    {}
    {}
    {}
    {}
    {\itshape\color{magenta}}
    {.}
    { }
    {\thmname{#1}\thmnote{ #3}}
\theoremstyle{solution}
\newtheorem*{solution}{Solution}

\Crefformat{exercise}{#2Exercise 3.D.#1#3}

\newcommand{\poly}{\mathcal{P}}
\newcommand{\lmap}{\mathcal{L}}
\newcommand{\mat}{\mathcal{M}}
\newcommand{\ts}{\textsuperscript}
\newcommand{\Span}{\text{span}}
\newcommand{\Null}{\text{null\,}}
\newcommand{\Range}{\text{range\,}}
\newcommand{\quand}{\quad \text{and} \quad}
\newcommand{\setcomp}[1]{#1^{\mathsf{c}}}
\newcommand{\N}{\mathbf{N}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\F}{\mathbf{F}}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}
% Swap the definition of \abs* and \norm*, so that \abs
% and \norm resizes the size of the brackets, and the 
% starred version does not.
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
%
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\setlist[enumerate,1]{label={(\alph*)}}

\begin{document}

\section{Section 3.D Exercises}

Exercises with solutions from Section 3.D of \hyperlink{ladr}{[LADR]}.

\begin{exercise}
\label{ex:1}
    Suppose \( T \in \lmap(U, V) \) and \( S \in \lmap(V, W) \) are both invertible linear maps. Prove that \( ST \in \lmap(U, W) \) is invertible and that \( (ST)^{-1} = T^{-1} S^{-1} \).
\end{exercise}

\begin{solution}
    Observe that for any \( u \in U \) we have
    \[
        (T^{-1} S^{-1} S T)(u) = T^{-1}(S^{-1}(S(Tu))) = T^{-1}(Tu) = u.
    \]
    Thus \( T^{-1} S^{-1} S T \) is the identity on \( U \). Similarly, for any \( w \in W \) we have
    \[
        (S T T^{-1} S^{-1})(w) = S(T(T^{-1}(S^{-1}w))) = S(S^{-1}w) = w.
    \]
    Thus \( S T T^{-1} S^{-1} \) is the identity on \( W \). It follows that \( ST \) is invertible and that \( (ST)^{-1} = T^{-1} S^{-1} \).
\end{solution}

\begin{exercise}
\label{ex:2}
    Suppose \( V \) is finite-dimensional and \( \dim V > 1 \). Prove that the set of noninvertible operators on \( V \) is not a subspace of \( \lmap(V) \).
\end{exercise}

\begin{solution}
    Let \( X = \{ T \in \lmap(V) : T \text{ is not invertible} \} \). Consider \href{https://lew98.github.io/Mathematics/LADR_Section_3_B_Exercises.pdf}{Exercise 3.B.7}, taking \( W = V, n = m, \) and \( w_j = v_j \). The linear maps \( S \) and \( T \) defined there fail to be injective and thus belong to \( X \), but the map \( S + T \) is simply the identity on \( V \) and hence belongs to \( X \). Thus \( X \) is not closed under addition and so is not a subspace of \( \lmap(V) \).
\end{solution}

\begin{exercise}
\label{ex:3}
    Suppose \( V \) is finite-dimensional, \( U \) is a subspace of \( V \), and \( S \in \lmap(U, V) \). Prove there exists an invertible operator \( T \in \lmap(V) \) such that \( Tu = Su \) for every \( u \in U \) if and only if \( S \) is injective.
\end{exercise}

\begin{solution}
    Suppose there exists such an operator \( T \) and let \( u \in U \) be such that \( Su = 0 \). Then \( Tu = Su = 0 \) and thus \( u = 0 \) since \( T \) is injective. Hence \( \Null S = \{ 0 \} \) and we see that \( S \) is injective.

    Now suppose that \( S \) is injective. Let \( u_1, \ldots, u_m \) be a basis of \( U \), which we extend to a basis \( u_1, \ldots, u_m, x_1, \ldots, x_n \) of \( V \). Since \( S \) is injective, \href{https://lew98.github.io/Mathematics/LADR_Section_3_B_Exercises.pdf}{Exercise 3.B.9} implies that \( Su_1, \ldots, Su_m \) is linearly independent in \( V \) and thus can be extended to a basis \( Su_1, \ldots, Su_m, y_1, \ldots, y_n \) of \( V \). Define a linear map \( T : V \to V \) by
    \[
        Tu_j = Su_j \text{ for } 1 \leq j \leq m \quand Tx_j = y_j \text{ for } 1 \leq j \leq n.
    \]
    Evidently, \( T \) extends \( S \). Suppose \( v \in V \) is such that \( Tv = 0 \). There are scalars \( a_1, \ldots, a_m, b_1, \ldots, b_n \) such that \( v = \sum_{j=1}^m a_j u_j + \sum_{k=1}^n b_k x_k \). Then:
    \[
        0 = Tv = T \left( \sum_{j=1}^m a_j u_j + \sum_{k=1}^n b_k x_k \right) = \sum_{j=1}^m a_j Tu_j + \sum_{k=1}^n b_k Tx_k = \sum_{j=1}^m a_j Su_j + \sum_{k=1}^n b_k y_k.
    \]
    The linear independence of \( Su_1, \ldots, Su_m, y_1, \ldots, y_n \) implies that \( a_1 = \cdots = a_m = b_1 = \cdots = b_n = 0 \) and thus that \( v = 0 \). Hence \( T \) is injective and 3.69 allows us to conclude that \( T \) is invertible.
\end{solution}

\begin{exercise}
\label{ex:4}
    Suppose \( W \) is finite-dimensional and \( T_1, T_2 \in \lmap(V, W) \). Prove that \( \Null T_1 = \Null T_2 \) if and only if there exists an invertible operator \( S \in \lmap(W) \) such that \( T_1 = ST_2 \).
\end{exercise}

\begin{solution}
    Suppose there exists such an operator \( S \) and suppose \( v \in \Null T_2 \). Then \( T_1 v = S(T_2v) = S(0) = 0 \) and thus \( \Null T_2 \subseteq \Null T_1 \). Since \( S \) is invertible, we have \( T_2 = S^{-1} T_1 \) and we may similarly derive that \( \Null T_1 \subseteq \Null T_2 \). Thus \( \Null T_1 = \Null T_2 \).

    Now suppose that \( \Null T_1 = \Null T_2 \). Since \( W \) is finite-dimensional, \( \Range T_2 \) is also finite-dimensional; let \( T_2 v_1, \ldots, T_2 v_m \) be a basis of \( \Range T_2 \), for some vectors \( v_1, \ldots, v_m \) in \( V \). Define a linear map \( S' : \Range T_2 \to W \) by \( S'(T_2 v_j) = T_1 v_j \). For any \( v \in V \), there are scalars \( a_1, \ldots, a_m \) such that \( T_2v = a_1 T_2v_1 + \cdots + a_m T_2v_m \). This gives
    \begin{multline*}
        T_2v = T_2(a_1 v_1 + \cdots + a_m v_m) \iff v - (a_1 v_1 + \cdots + a_m v_m) \in \Null T_2 \\ \iff v - (a_1 v_1 + \cdots + a_m v_m) \in \Null T_1, 
    \end{multline*}
    where we have used the assumption that \( \Null T_1 = \Null T_2 \). Since \( v - (a_1 v_1 + \cdots + a_m v_m) \in \Null T_1 \), we have that \( T_1v = a_1 T_1v_1 + \cdots + a_m T_1v_m \). It follows that
    \[
        S'(T_2v) = a_1 S'(T_2v_1) + \cdots + a_m S'(T_2v_m) = a_1 T_1v_1 + \cdots + a_m T_mv_m = T_1v.
    \]
    Hence \( T_1 = S'T_2 \).
    
    Now we claim that \( S' \) is injective. Suppose that \( T_2v \in \Range T_2 \) is such that \( S'(T_2v) = 0 \). Then \( T_1v = 0 \), so that \( v \in \Null T_1 = \Null T_2 \). Thus \( T_2v = 0 \) and we see that \( \Null S' = \{ 0 \} \), i.e.\ that \( S' \) is injective. \Cref{ex:3} now implies that there is an invertible operator \( S \in \lmap(W) \) which extends \( S' \) and hence satisfies \( T_1 = ST_2 \).
\end{solution}

\begin{exercise}
\label{ex:5}
    Suppose \( V \) is finite-dimensional and \( T_1, T_2 \in \lmap(V, W) \). Prove that \( \Range T_1 = \Range T_2 \) if and only if there exists an invertible operator \( S \in \lmap(V) \) such that \( T_1 = T_2 S \).
\end{exercise}

\begin{solution}
    Suppose there exists such an operator \( S \). If \( T_1v \in \Range T_1 \) for some \( v \in V \), then \( T_1v = T_2(Sv) \), so that \( T_1v \in \Range T_2 \) also. If \( T_2v \in \Range T_2 \) for some \( v \in V \), then \( T_2v = T_1(S^{-1}v) \), so that \( T_2v \in \Range T_1 \) also. Thus \( \Range T_1 = \Range T_2 \).

    Now suppose that \( \Range T_1 = \Range T_2 \). Let \( u_1, \ldots, u_m \) be a basis of \( \Null T_1 \), which we extend to a basis \( u_1, \ldots, u_m, x_1, \ldots, x_n \) of \( V \). If we let \( X := \Span(x_1, \ldots, x_n) \), we then have \( V = \Null T_1 \oplus X \). 
    
    The restriction of \( T_1 \) to \( X \) is injective since \( \Null T_1 \cap X = \{ 0 \} \), so \href{https://lew98.github.io/Mathematics/LADR_Section_3_B_Exercises.pdf}{Exercise 3.B.9} implies that the list \( T_1 x_1, \ldots, T_1 x_n \) is linearly independent. Furthermore, \href{https://lew98.github.io/Mathematics/LADR_Section_3_B_Exercises.pdf}{Exercise 3.B.10} implies that the list
    \[
        T_1 u_1, \ldots, T_1 u_m, T_1 x_1, \ldots, T_1 x_n
    \]
    spans \( \Range T_1 \). Since each \( T_1 u_j = 0 \), we can discard these vectors to see that the list \( T_1 x_1, \ldots, T_1 x_n \) spans \( \Range T_1 \). We have now shown that \( T_1 x_1, \ldots, T_1 x_n \) is a basis of \( \Range T_1 \).

    By assumption, we have \( \Range T_1 = \Range T_2 \), and thus there are vectors \( y_1, \ldots, y_n \) in \( V \) such that \( T_1 x_j = T_2 y_j \). Since the list \( T_2 y_1, \ldots, T_2 y_n \) is linearly independent, \href{https://lew98.github.io/Mathematics/LADR_Section_3_A_Exercises.pdf}{Exercise 3.A.4} shows that the list \( y_1, \ldots, y_n \) is linearly independent. Let \( v_1, \ldots, v_m \) be a basis of \( \Null T_2 \) (since \( \Range T_1 = \Range T_2 \), the Fundamental Theorem of Linear Maps (3.22) implies that \( \dim \Null T_1 = \dim \Null T_2 \), so that this basis is also of length \( m \)). As the proof of 3.22 shows, \( v_1, \ldots, v_m, y_1, \ldots, y_n \) must be a basis of \( V \).

    Define a linear map \( S : V \to V \) by
    \[
        Su_j = v_j \text{ for } 1 \leq j \leq m \quand Sx_j = y_j \text{ for } 1 \leq j \leq n.
    \]
    If \( v = a_1 u_1 + \cdots + a_{m+n} x_n \) is such that \( Sv = 0 \), then
    \[
        0 = a_1 Su_1 + \cdots + a_{m+n} Sx_n = a_1 v_1 + \cdots + a_{m+n} y_n.
    \]
    The linear independence of the basis \( v_1, \ldots, y_n \) then implies that \( a_1 = \cdots = a_{m+n} = 0 \) and hence that \( v = 0 \). Thus \( \Null S = \{ 0 \} \) and we see that \( S \) is injective; 3.69 allows us to conclude that \( S \) is an invertible operator. Furthermore, we have \( T_1 = T_2 S \). Indeed, for any \( v = \sum_{j=1}^m a_j u_j + \sum_{k=1}^n b_j x_j \) in \( V \), we have
    \begin{multline*}
        (T_2S)(v) = \sum_{j=1}^m a_j T_2(Su_j) + \sum_{k=1}^n b_j T_2(Sx_j) = \sum_{j=1}^m a_j T_2v_j + \sum_{k=1}^n b_j T_2y_j \\ = \sum_{j=1}^m a_j T_1u_j + \sum_{k=1}^n b_j T_1x_j = T_1 \left( \sum_{j=1}^m a_j u_j + \sum_{k=1}^n b_j x_j \right) = T_1v.
    \end{multline*}
\end{solution}

\begin{exercise}
\label{ex:6}
    Suppose \( V \) and \( W \) are finite-dimensional and \( T_1, T_2 \in \lmap(V, W) \). Prove that there exist invertible operators \( R \in \lmap(V) \) and \( S \in \lmap(W) \) such that \( T_1 = ST_2R \) if and only if \( \dim \Null T_1 = \dim \Null T_2 \).
\end{exercise}

\begin{solution}
    Suppose there exist such operators \( R \) and \( S \), so that \( T_1 = ST_2R \). Notice that this gives us \( T_2 = S^{-1}T_1R^{-1} \). \href{https://lew98.github.io/Mathematics/LADR_Section_3_B_Exercises.pdf}{Exercise 3.B.22} now implies that
    \begin{gather*}
        \dim \Null T_1 = \dim \Null ST_2R \leq \dim \Null S + \dim \Null T_2 + \dim \Null R = \dim \Null T_2, \\[2mm]
        \dim \Null T_2 = \dim \Null S^{-1}T_1R^{-1} \leq \dim \Null S^{-1} + \dim \Null T_1 + \dim \Null R^{-1} = \dim \Null T_1,
    \end{gather*}
    where we have used that each invertible linear map \( R, S, R^{-1}, \) and \( S^{-1} \) are injective and hence have trivial null space. These two inequalities combine to give us \( \dim \Null T_1 = \dim \Null T_2 \).

    Now suppose that \( \dim \Null T_1 = \dim \Null T_2 \). Let \( u_1, \ldots, u_m \) be a basis of  \( \Null T_1 \), which we extend to a basis \( u_1, \ldots, u_m, x_1, \ldots, x_n \) of \( V \), and let \( v_1, \ldots, v_m \) be a basis of \( \Null T_2 \), which we extend to a basis \( v_1, \ldots, v_m, y_1, \ldots, y_n \) of \( V \). Define an operator \( R : V \to V \) by
    \[
        Ru_j = v_j \text{ for } 1 \leq j \leq m \quand Rx_j = y_j \text{ for } 1 \leq j \leq n.
    \]
    As in the solution to \Cref{ex:5}, this operator must be invertible since it maps a basis to a basis. We claim that \( \Null T_1 = \Null T_2R \). Suppose that \( u \in \Null T_1 \), so that \( u = a_1 u_1 + \cdots + a_m u_m \) for some scalars \( a_1, \ldots, a_m \). Then
    \[
        T_2(Ru) = T_2(a_1 Ru_1 + \cdots + a_m Ru_m) = T_2(a_1 v_1 + \cdots + a_m v_m) = 0.
    \]
    Thus \( \Null T_1 \subseteq \Null T_2R \). Suppose that \( v \in \Null T_2R \), i.e.\ \( T_2(Rv) = 0 \). Then \( Rv \in \Null T_2 \), so that \( Rv = a_1 v_1 + \cdots + a_m v_m \) for some scalars \( a_1, \ldots, a_m \). This gives us
    \[
        Rv = a_1 Ru_1 + \cdots + a_m Ru_m = R(a_1 u_1 + \cdots + a_m u_m),
    \]
    which implies that \( v = a_1 u_1 + \cdots + a_m u_m \in \Null T_1 \) since \( R \) is injective. Thus \( \Null T_1 = \Null T_2R \), as claimed.

    We may now appeal to \Cref{ex:4} to obtain an invertible operator \( S \) such that \( T_1 = ST_2R \).
\end{solution}

\begin{exercise}
\label{ex:7}
    Suppose \( V \) and \( W \) are finite-dimensional. Let \( v \in V \). Let
    \[
        E = \{ T \in \lmap(V, W) : Tv = 0 \}.
    \]
    \begin{enumerate}
        \item Show that \( E \) is a subspace of \( \lmap(V, W) \).

        \item Suppose \( v \neq 0 \). What is \( \dim E \)?
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item Suppose \( S, T \in E \) and \( \lambda \in \F \). Then
        \[
            (\lambda S + T)(v) = \lambda Sv + Tv = 0.
        \]
        Thus \( \lambda S + T \in E \) and so \( E \) is a subspace of \( \lmap(V, W) \).

        \item Set \( v_1 := v \). Since \( v_1 \neq 0 \), we can extend this list to a basis \( v_1, \ldots, v_m \) of \( V \). Let \( w_1, \ldots, w_n \) be any basis of \( W \). By 3.60, the linear map \( \mat : \lmap(V, W) \to \F^{n,m} \) is an isomorphism, which restricts to an isomorphism \( \mat : E \to \mathcal{E} \), where \( \mathcal{E} = \{ \mat(T) : T \in E \} \); we use this notation in place of \( \mat(E) \) for obvious reasons. We claim that \( \mathcal{E} \) is the subspace of matrices whose entries in the first column are all 0. Indeed, if \( T \in E \), then \( Tv_1 = 0 \) and thus the entries in the first column of \( \mat(T) \) are all 0. Conversely, if \( A \) is an \(n\)-by-\(m\) matrix with entries \( A_{j,k} \) such that \( A_{\cdot,1} = 0 \), then the linear map \( T : V \to W \) defined by
        \[
            T v_k = A_{1,k} w_1 + \cdots + A_{n,k} w_n
        \]
        satisfies \( Tv_1 = 0 \) and hence belongs to \( E \); evidently we have \( \mat(T) = A \). Our claim follows.

        Using the same logic as in the proof of 3.40, which shows that \( \dim F^{n,m} = nm \), it is easily verified that \( \mathcal{E} \), and hence \( E \), has dimension \( (m-1)n \). In conclusion, we have
        \[
            \dim E = (\dim V - 1)(\dim W).
        \]
    \end{enumerate}
\end{solution}

\begin{exercise}
\label{ex:8}
    Suppose \( V \) is finite-dimensional and \( T : V \to W \) is a surjective linear map of \( V \) onto \( W \). Prove that there is a subspace \( U \) of \( V \) such that \( T|_U \) is an isomorphism of \( U \) onto \( W \). (Here \( T|_U \) means the function \( T \) restricted to \( U \). In other words, \( T|_U \) is the function whose domain is \( U \), with \( T|_U \) defined by \( T|_U(u) = Tu \) for every \( u \in U \).)
\end{exercise}

\begin{solution}
    By \href{https://lew98.github.io/Mathematics/LADR_Section_3_B_Exercises.pdf}{Exercise 3.B.12}, there exists a subspace \( U \) of \( V \) such that \( V = U \oplus \Null T \) and \( W = \Range T = \{ Tu : u \in U \} \). Thus the restriction \( T|_U : U \to W \) is surjective. Furthermore, since \( U \cap \Null T = \{ 0 \} \), we have \( \Null T|_U = \{ 0 \} \) and hence \( T|_U \) is also injective. We may conclude that \( T|_U \) is an invertible linear map, i.e.\ an isomorphism.
\end{solution}

\begin{exercise}
\label{ex:9}
    Suppose \( V \) is finite-dimensional and \( S, T \in \lmap(V) \). Prove that \( ST \) is invertible if and only if \( S \) and \( T \) are both invertible.
\end{exercise}

\begin{solution}
    If \( S \) and \( T \) are both invertible, then \( ST \) is invertible by \Cref{ex:1}. If either \( S \) or \( T \) fails to be invertible, then 3.69 implies that at least one of these maps is not surjective. Thus
    \[
        \min \{ \dim \Range S, \dim \Range T \} < \dim V.
    \]
    \href{https://lew98.github.io/Mathematics/LADR_Section_3_B_Exercises.pdf}{Exercise 3.B.23} then implies that \( \dim \Range ST < \dim V \) and hence that \( ST \) is not surjective. We may now apply 3.69 again to see that \( ST \) is not invertible.
\end{solution}

\begin{exercise}
\label{ex:10}
    Suppose \( V \) is finite-dimensional and \( S, T \in \lmap(V) \). Prove that \( ST = I \) if and only if \( TS = I \).
\end{exercise}

\begin{solution}
    Suppose that \( ST = I \). Then \( ST \) is invertible, so \Cref{ex:9} implies that \( S \) and \( T \) are both invertible. Applying \( S^{-1} \) on the left to both sides of \( ST = I \) shows that \( T = S^{-1} \); it follows that \( TS = S^{-1}S = I \). Reversing the roles of \( S \) and \( T \) in the preceding argument gives us the converse implication.
\end{solution}

\begin{exercise}
\label{ex:11}
    Suppose \( V \) is finite-dimensional and \( S, T, U \in \lmap(V) \) and \( STU = I \). Show that \( T \) is invertible and that \( T^{-1} = US \).
\end{exercise}

\begin{solution}
    Applying \Cref{ex:9} twice shows that each of \( S, T, \) and \( U \) are invertible operators. It follows that \( T = S^{-1} U^{-1} \) and hence by \Cref{ex:1} we have \( T^{-1} = US \).
\end{solution}

\begin{exercise}
\label{ex:12}
    Show that the result in the previous exercise can fail without the hypothesis that \( V \) is finite-dimensional.
\end{exercise}

\begin{solution}
    Let \( V \) be the infinite-dimensional vector space \( \R^{\infty} \) and set \( S = I \). Take \( T \) to be the left-shift operator and \( U \) to be the right-shift operator, i.e.\
    \[
        T(x_1, x_2, x_3, \ldots) = (x_2, x_3, \ldots) \quand U(x_1, x_2, x_3, \ldots) = (0, x_1, x_2, x_3, \ldots).
    \]
    Then \( STU = I \), but \( T \) is not invertible. Indeed, \( T \) fails to be injective, since \( T(1, 0, 0, \ldots) = 0 \).
\end{solution}

\begin{exercise}
\label{ex:13}
    Suppose \( V \) is a finite-dimensional vector space and \( R, S, T \in \lmap(V) \) are such that \( RST \) is surjective. Prove that \( S \) is injective.
\end{exercise}

\begin{solution}
    By 3.69, \( RST \) must be invertible. Applying \Cref{ex:9} twice shows that each of \( R, S, \) and \( T \) are invertible operators. Thus \( S \) is injective.
\end{solution}

\begin{exercise}
\label{ex:14}
    Suppose \( v_1, \ldots, v_n \) is a basis of \( V \). Prove that the map \( T : V \to \F^{n,1} \) defined by
    \[
        Tv = \mat(v)
    \]
    is an isomorphism of \( V \) onto \( \F^{n,1} \); here \( \mat(v) \) is the matrix of \( v \in V \) with respect to the basis \( v_1, \ldots, v_n \).
\end{exercise}

\begin{solution}
    First, let us show that \( T \) is linear. Suppose \( u, v \in V \), so that there are scalars \( a_1, \ldots, a_n \) and \( b_1, \ldots, b_n \) such that
    \[
        u = a_1 v_1 + \cdots + a_n v_n \quand v = b_1 v_1 + \cdots + b_n v_n,
    \]
    and let \( \lambda \in \F \) be given. Then
    \[
        u + v = (a_1 + b_1) v_1 + \cdots + (a_n + b_n) v_n \quand \lambda u = (\lambda a_1) v_1 + \cdots + (\lambda a_n) v_n.
    \]
    Thus
    \[
        Tu = \begin{pmatrix}
            a_1 \\ \vdots \\ a_n
        \end{pmatrix}, \quad
        Tv = \begin{pmatrix}
            b_1 \\ \vdots \\ b_n
        \end{pmatrix}, \quad
        T(u + v) = \begin{pmatrix}
            a_1 + b_1 \\ \vdots \\ a_n + b_n
        \end{pmatrix} \quand
        T(\lambda u) = \begin{pmatrix}
            \lambda a_1 \\ \vdots \\ \lambda a_n
        \end{pmatrix}.
    \]
    Hence \( T(u + v) = Tu + Tv \) and \( T(\lambda u) = \lambda Tu \) and thus \( T \) is linear. Furthermore, \( T \) is surjective, since for any scalars \( a_1, \ldots, a_n \) we have
    \[
        T(a_1 v_1 + \cdots + a_n v_n) = \begin{pmatrix}
            a_1 \\ \vdots \\ a_n
        \end{pmatrix}.
    \]
    Since \( V \) and \( \F^{n,1} \) are both of dimension \( n \), 3.69 allows us to conclude that \( T \) is an isomorphism.
\end{solution}

\begin{exercise}
\label{ex:15}
    Prove that every linear map from \( \F^{n,1} \) to \( \F^{m,1} \) is given by a matrix multiplication. In other words, prove that if \( T \in \lmap(\F^{n,1}, \F^{m,1}) \), then there exists an \(m\)-by-\(n\) matrix \( A \) such that \( Tx = Ax \) for every \( x \in \F^{n,1} \).
\end{exercise}

\begin{solution}
    Let \( e_j \) be the column vector with a 1 in the \(j\)\ts{th} row and a 0 in each other row. Take \( e_1, \ldots, e_n \) as a basis of \( \F^{n,1} \) and \( e_1, \ldots, e_m \) as a basis of \( \F^{m,1} \). Then for each \( x \in \F^{n,1} \) we have \( \mat(x) = x \) and \( \mat(Tx) = Tx \). By 3.65, the desired matrix \( A \) is \( \mat(T) \).
\end{solution}

\begin{exercise}
\label{ex:16}
    Suppose \( V \) is finite-dimensional and \( T \in \lmap(V) \). Prove that \( T \) is a scalar multiple of the identity if and only if \( ST = TS \) for every \( S \in \lmap(V) \).
\end{exercise}

\begin{solution}
    Suppose that \( T \) is a scalar multiple of the identity, say \( T = \lambda I \) for some \( \lambda \in \F \). Let \( S \in \lmap(V) \) and \( v \in V \) be given. Then
    \[
        S(Tv) = S((\lambda I)(v)) = S(\lambda v) = \lambda (Sv) = (\lambda I)(Sv) = T(Sv).
    \]
    Thus \( ST = TS \).

    Now suppose that \( ST = TS \) for every \( S \in \lmap(V) \). Let \( v_1, \ldots, v_m \) be a basis of \( V \) and for \( 1 \leq j, k \leq m \), define \( S_{j,k} : V \to V \) by
    \[
        S_{j,k} v_i = \begin{cases}
            v_k & \text{if } i = j, \\
            v_j & \text{if } i = k, \\
            0 & \text{otherwise}. 
        \end{cases}
    \]
    Let \( A = \mat(T) \), so that
    \[
        Tv_j = A_{1,j} v_1 + \cdots + A_{m,j} v_m.
    \]
    For any \( 1 \leq j \leq m \), observe that
    \begin{gather*}
        S_{j,j}(Tv_j) = S_{j,j}(A_{1,j} v_1 + \cdots + A_{m,j} v_m) = A_{1,j} S_{j,j}v_1 + \cdots + A_{m,j} S_{j,j}v_m = A_{j,j} v_j, \\[2mm]
        T(S_{j,j}v_j) = Tv_j = A_{1,j} v_1 + \cdots + A_{m,j} v_m.
    \end{gather*}
    By assumption we have \( S_{j,j}(Tv_j) = T(S_{j,j}v_j) \), so
    \[
        A_{j,j} v_j = A_{1,j} v_1 + \cdots + A_{m,j} v_m \iff A_{1,j} v_1 + \cdots + A_{j-1,j} v_{j-1} + A_{j+1,j} v_{j+1} + \cdots + A_{m,j} v_m = 0.
    \]
    Thus by linear independence we have
    \[
        A_{1,j} = \cdots = A_{j-1,j} = A_{j+1,j} = \cdots = A_{m,j} = 0.
    \]
    In other words, each non-diagonal entry of \( A \) is 0, so that \( Tv_j = A_{j,j} v_j \) for each \( 1 \leq j \leq m \). Now suppose that \( 1 \leq j < k \leq m \). Then
    \[
        S_{j,k}(Tv_j) = S_{j,k}(A_{j,j} v_j) = A_{j,j} S_{j,k} v_j = A_{j,j} v_k \quand T(S_{j,k} v_j) = Tv_k = A_{k,k} v_k.
    \]
    By assumption these two must be equal, i.e.\ \( A_{j,j} v_k = A_{k,k} v_k \), and so \( A_{j,j} = A_{k,k} \) since \( v_k \neq 0 \). Thus, letting \( \lambda = A_{1,1} \), we have shown that \( Tv_j = \lambda v_j \) for each basis vector \( v_j \). It follows that \( T = \lambda I \).
\end{solution}

\begin{exercise}
\label{ex:17}
    Suppose \( V \) is finite-dimensional and \( \mathcal{E} \) is a subspace of \( \lmap(V) \) such that \( ST \in \mathcal{E} \) and \( TS \in \mathcal{E} \) for all \( S \in \lmap(V) \) and all \( T \in \mathcal{E} \). Prove that \( \mathcal{E} = \{ 0 \} \) or \( \mathcal{E} = \lmap(V) \).
\end{exercise}

\begin{solution}
    If \( \mathcal{E} = \{ 0 \} \) then evidently \( \mathcal{E} \) has the desired properties, so it will suffice to show that if there is some \( T \in \mathcal{E} \) with \( T \neq 0 \), then \( \mathcal{E} = \lmap(V) \). Let \( v_1, \ldots, v_m \) be a basis of \( V \). Define a linear map \( E_{i,j} \in \lmap(V) \) by
    \[
        E_{i,j} v_k = \delta^i_k v_j,
    \]
    where \( \delta^i_k \) is the \href{https://en.wikipedia.org/wiki/Kronecker_delta}{Kronecker delta}, i.e.\ \( \delta^i_k = 1 \) if \( i = k \) and \( \delta^i_k = 0 \) otherwise. In other words, \( E_{i,j} \) sends \( v_i \) to \( v_j \) and each other basis vector to 0.
    
    Since \( T \neq 0 \), there must be some \( Tv_p \neq 0 \). Suppose that
    \[
        Tv_p = a_1 v_1 + \cdots + a_m v_m.
    \]
    Since \( Tv_p \neq 0 \), there is some \( a_r \neq 0 \). For each \( 1 \leq i \leq m \), we have
    \[
        a_r^{-1} E_{r,i} T E_{i,p} v_k = a_r^{-1} E_{r,i} T (\delta^i_k v_p) = a_r^{-1} \delta^i_k E_{r,i} (a_1 v_1 + \cdots + a_m v_m) = \delta^i_k v_i.
    \]
    Set \( L := a_r^{-1} (E_{r,1} T E_{1,p} + \cdots + E_{r,m} T E_{m,p}) \). By the equality above, we then have \( Lv_k = v_k \), i.e.\ \( L \) is the identity map on \( V \). By assumption, we have \( E_{r,i} T E_{i,p} \in \mathcal{E} \) for each \( 1 \leq i \leq m \). Since \( \mathcal{E} \) is a subspace of \( \lmap(V) \), it follows that \( L \in \mathcal{E} \). Then for any \( S \in \lmap(V) \), we have \( SL = S \in \mathcal{E} \) and thus \( \mathcal{E} = \lmap(V) \).
\end{solution}

\begin{exercise}
\label{ex:18}
    Show that \( V \) and \( \lmap(\F, V) \) are isomorphic vector spaces.
\end{exercise}

\begin{solution}
    Given \( v \in V \), define a linear map \( T_v : \F \to V \) by \( T_v(1) = v \) and a map \( \Phi : V \to \lmap(\F, V) \) by \( \Phi(v) = T_v \). Showing that \( \Phi \) is linear amounts to showing that for \( u, v \in V \) and \( \lambda \in \F \), one has \( T_{u + \lambda v} = T_u + \lambda T_v \). Observe that
    \[
        T_{u + \lambda v}(1) = u + \lambda v = T_u(1) + \lambda T_v(1) = (T_u + \lambda T_v)(1).
    \]
    The uniqueness part of 3.5 now implies that \( T_{u + \lambda v} = T_u + \lambda T_v \) and thus \( \Phi \) is linear.

    The map \( \Phi \) is injective. Indeed, if \( v \in V \) is such that \( \Phi(v) = T_v = 0 \), then in particular \( T_v(1) = v = 0 \) and so \( \Null \Phi = \{ 0 \} \). By 3.61 we have \( \dim V = \dim \lmap(\F, V) \) and so 3.69 allows us to conclude that \( \Phi \) is an isomorphism.
\end{solution}

\begin{exercise}
\label{ex:19}
    Suppose \( T \in \lmap(\poly(\R)) \) is such that \( T \) is injective and \( \deg Tp \leq \deg p \) for every nonzero polynomial \( p \in \poly(\R) \).
    \begin{enumerate}
        \item Prove that \( T \) is surjective.
        
        \item Prove that \( \deg Tp = \deg p \) for every nonzero \( p \in \poly(\R) \).
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item For each positive integer \( m \), consider the restriction \( T_m : \poly_m(\R) \to \poly(\R) \) given by \( T_m p = Tp \); since \( T \) is injective, each \( T_m \) is also injective. The hypothesis \( \deg Tp \leq \deg p \) shows that this restriction actually maps into \( \poly_m(\R) \) and so 3.69 implies that each \( T_m \) is an isomorphism. Then for any \( p \in \poly(\R) \), let \( m = \deg p \). As we just showed, there exists some \( q \in \poly_m(\R) \) such that \( T_m q = Tq = p \).

        \item We will prove this by induction on the degree of \( p \). Let \( P(n) \) be the statement that for all polynomials \( p \) of degree \( n \), we have \( \deg Tp = n \). For the base case \( P(0) \), let \( p \) be a non-zero constant polynomial. Then since \( T \) is injective, we have \( Tp \neq 0 \) and thus \( \deg Tp = 0 \).

        Now suppose that \( P(n) \) is true for some \( n \) and let \( p \) be a polynomial of degree \( n + 1 \). Suppose by way of contradiction that \( \deg Tp < n + 1 \). Then as we showed in part (a), there exists a polynomial \( q \) with \( \deg q \leq n \) such that \( Tq = Tp \). The injectivity of \( T \) implies that \( p = q \), but this is a contradiction since \( p \) and \( q \) have different degrees. Thus \( \deg Tp = n + 1 \) and so \( P(n+1) \) holds. This completes the induction step and the proof.
    \end{enumerate}
\end{solution}

\begin{exercise}
\label{ex:20}
    Suppose \( n \) is a positive integer and \( A_{i,j} \in \F \) for \( i, j = 1, \ldots, n \). Prove that the following are equivalent (note that in both parts below, the number of equations equals the number of variables):
    \begin{enumerate}
        \item The trivial solution \( x_1 = \cdots = x_n = 0 \) is the only solution to the homogeneous system of equations
        \begin{gather*}
            \sum_{k=1}^n A_{1,k} x_k = 0 \\
            \vdots \\
            \sum_{k=1}^n A_{n,k} x_k = 0.
        \end{gather*}

        \item For every \( c_1, \ldots, c_n \in \F \), there exists a solution to the system of equations
        \begin{gather*}
            \sum_{k=1}^n A_{1,k} x_k = c_1 \\
            \vdots \\
            \sum_{k=1}^n A_{n,k} x_k = c_n.
        \end{gather*}
    \end{enumerate}
\end{exercise}

\begin{solution}
    As in Example 3.25, we can rephrase this question in terms of a linear map. Define \( T : \F^n \to \F^n \) by
    \[
        T(x_1, \ldots, x_n) = \left( \sum_{k=1}^n A_{1,k} x_k, \ldots, \sum_{k=1}^n A_{n,k} x_k \right).
    \]
    Then (a) is equivalent to the injectivity of \( T \) and (b) is equivalent to the surjectivity of \( T \). The equivalence of (a) and (b) then follows from 3.69.
\end{solution}

\noindent \hrulefill

\noindent \hypertarget{ladr}{\textcolor{blue}{[LADR]} Axler, S. (2015) \textit{Linear Algebra Done Right.} 3\ts{rd} edition.}

\end{document}